{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numpy==2.0.0 scikit-learn==1.6.0 --force-reinstall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "id": "U1Fr1yOEK_Bt",
        "outputId": "fe99c908-0283-4c3d-8772-4c8c0bb1e861"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==2.0.0\n",
            "  Using cached numpy-2.0.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Collecting scikit-learn==1.6.0\n",
            "  Downloading scikit_learn-1.6.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting scipy>=1.6.0 (from scikit-learn==1.6.0)\n",
            "  Using cached scipy-1.16.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
            "Collecting joblib>=1.2.0 (from scikit-learn==1.6.0)\n",
            "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn==1.6.0)\n",
            "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Using cached numpy-2.0.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.0 MB)\n",
            "Downloading scikit_learn-1.6.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
            "Using cached scipy-1.16.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.7 MB)\n",
            "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: threadpoolctl, numpy, joblib, scipy, scikit-learn\n",
            "  Attempting uninstall: threadpoolctl\n",
            "    Found existing installation: threadpoolctl 3.6.0\n",
            "    Uninstalling threadpoolctl-3.6.0:\n",
            "      Successfully uninstalled threadpoolctl-3.6.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.0\n",
            "    Uninstalling numpy-2.0.0:\n",
            "      Successfully uninstalled numpy-2.0.0\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.5.2\n",
            "    Uninstalling joblib-1.5.2:\n",
            "      Successfully uninstalled joblib-1.5.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.3\n",
            "    Uninstalling scipy-1.16.3:\n",
            "      Successfully uninstalled scipy-1.16.3\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.5.2\n",
            "    Uninstalling scikit-learn-1.5.2:\n",
            "      Successfully uninstalled scikit-learn-1.5.2\n",
            "Successfully installed joblib-1.5.2 numpy-2.0.0 scikit-learn-1.6.0 scipy-1.16.3 threadpoolctl-3.6.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "joblib",
                  "numpy",
                  "scipy",
                  "sklearn",
                  "threadpoolctl"
                ]
              },
              "id": "244a5b134cbd47aea659cb8836a554ad"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sklearn\n",
        "\n",
        "print(\"NumPy:\", np.__version__)\n",
        "print(\"Sklearn:\", sklearn.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPgNVFZ2LQG6",
        "outputId": "c201ee6d-e3fa-48b8-904d-f18d9390b9a4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy: 2.0.0\n",
            "Sklearn: 1.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "confusion_matrix, roc_auc_score, roc_curve, classification_report)\n",
        "import joblib\n",
        "RANDOM_STATE = 42"
      ],
      "metadata": {
        "id": "4gHGeRxhLTRp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "print('If you have a local CSV, run the next line to upload it to Colab runtime:')\n",
        "print('\\nIf you want to download an example dataset (Titanic), run this cell:')\n",
        "def load_example(name='titanic'):\n",
        "    if name=='titanic':\n",
        "        url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "        return pd.read_csv(url)\n",
        "    elif name=='heart':\n",
        "        url = 'https://raw.githubusercontent.com/amarbudhiraj/Heart-Disease-Prediction-using-Machine-Learning/master/heart.csv'\n",
        "        return pd.read_csv(url)\n",
        "    else:\n",
        "        raise ValueError('example not found')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66UmivfbLYkT",
        "outputId": "21fa3565-4523-4e5c-f3d9-d61bacebdb68"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If you have a local CSV, run the next line to upload it to Colab runtime:\n",
            "\n",
            "If you want to download an example dataset (Titanic), run this cell:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def quick_check(df):\n",
        "    print('Rows, cols:', df.shape)\n",
        "    display(df.head())\n",
        "    display(df.describe(include='all').T)\n",
        "    print('\\nMissing values per column:')\n",
        "    display(df.isnull().sum().sort_values(ascending=False).head(20))"
      ],
      "metadata": {
        "id": "qCayYUBrM5QK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_duplicates(df):\n",
        "    before = df.shape[0]\n",
        "    df = df.drop_duplicates()\n",
        "    after = df.shape[0]\n",
        "    print(f'Removed {before-after} duplicates')\n",
        "    return df\n",
        "def fix_dtypes(df, convert_dict=None):\n",
        "    if convert_dict is None:\n",
        "        return df\n",
        "    for c,t in convert_dict.items():\n",
        "        try:\n",
        "            if t=='datetime':\n",
        "                df[c] = pd.to_datetime(df[c], errors='coerce')\n",
        "            else:\n",
        "                df[c] = df[c].astype(t)\n",
        "            print(f'Converted {c} to {t}')\n",
        "        except Exception as e:\n",
        "            print(f'Could not convert {c}:', e)\n",
        "    return df\n",
        "\n",
        "\n",
        "def impute_missing(df, strategy_num='median', strategy_cat='most_frequent', drop_thresh=0.5):\n",
        "    n = df.shape[0]\n",
        "    drop_cols = [c for c in df.columns if df[c].isnull().sum()/n>drop_thresh]\n",
        "    if drop_cols:\n",
        "        print('Dropping high-missing columns:', drop_cols)\n",
        "        df = df.drop(columns=drop_cols)\n",
        "\n",
        "\n",
        "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    cat_cols = df.select_dtypes(exclude=[np.number, 'datetime']).columns.tolist()\n",
        "\n",
        "\n",
        "    if num_cols:\n",
        "        imputer_num = SimpleImputer(strategy=strategy_num)\n",
        "        df[num_cols] = imputer_num.fit_transform(df[num_cols])\n",
        "    if cat_cols:\n",
        "        imputer_cat = SimpleImputer(strategy=strategy_cat, fill_value='Missing')\n",
        "        df[cat_cols] = imputer_cat.fit_transform(df[cat_cols])\n",
        "\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def cap_outliers_iqr(df, cols=None):\n",
        "    if cols is None:\n",
        "        cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    for c in cols:\n",
        "        q1 = df[c].quantile(0.25)\n",
        "        q3 = df[c].quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "        low = q1 - 1.5*iqr\n",
        "        high = q3 + 1.5*iqr\n",
        "        df[c] = np.where(df[c] < low, low, df[c])\n",
        "        df[c] = np.where(df[c] > high, high, df[c])\n",
        "    print('Capped outliers using IQR for', cols)\n",
        "    return df"
      ],
      "metadata": {
        "id": "qejETaMCNEMm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eda_plots(df, target=None, figsize=(12,8)):\n",
        "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    cat_cols = df.select_dtypes(exclude=[np.number, 'datetime']).columns.tolist()\n",
        "\n",
        "\n",
        "    # 1. Histograms for numeric\n",
        "    df[num_cols].hist(bins=20, figsize=(14,10))\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "\n",
        "    # 2. Heatmap (correlation)\n",
        "    plt.figure(figsize=(10,8))\n",
        "    sns.heatmap(df[num_cols].corr(), annot=True, fmt='.2f', cmap='coolwarm')\n",
        "    plt.title('Correlation matrix (numeric)')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # 3. Boxplots for numeric to see outliers\n",
        "    for c in num_cols:\n",
        "        plt.figure(figsize=(6,2))\n",
        "        sns.boxplot(x=df[c])\n",
        "        plt.title(f'Boxplot - {c}')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    # 4. If target given, show relationship plots\n",
        "    if target and target in df.columns:\n",
        "        for c in num_cols:\n",
        "            if c==target: continue\n",
        "            plt.figure(figsize=(6,4))\n",
        "            sns.scatterplot(data=df, x=c, y=target)\n",
        "            plt.title(f'{c} vs {target}')\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "        for c in cat_cols[:6]:\n",
        "            plt.figure(figsize=(8,4))\n",
        "            sns.countplot(data=df, x=c, hue=target)\n",
        "            plt.title(f'{c} counts by {target}')\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "def basic_insights(df, target=None, top_n=10):\n",
        "    insights = []\n",
        "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    if target and target in df.columns:\n",
        "        corr = df[num_cols].corr()[target].drop(target).abs().sort_values(ascending=False)\n",
        "        insights.append('Top numeric correlates with target: ' + ', '.join(corr.head(5).index.tolist()))\n",
        "    nulls = df.isnull().sum()\n",
        "    if nulls.sum()>0:\n",
        "        insights.append('Columns with missing values: ' + ', '.join(nulls[nulls>0].index.tolist()))\n",
        "    insights.append('Number of rows and columns: ' + str(df.shape))\n",
        "    return insights"
      ],
      "metadata": {
        "id": "Ni1FHCuGNXEr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "def create_feature_pipeline(df, categorical_thresh=10, target=None):\n",
        "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    cat_cols = df.select_dtypes(exclude=[np.number, 'datetime']).columns.tolist()\n",
        "    if target and target in num_cols:\n",
        "        num_cols.remove(target)\n",
        "    if target and target in cat_cols:\n",
        "        cat_cols.remove(target)\n",
        "    ohe_cols = [c for c in cat_cols if df[c].nunique()<=categorical_thresh]\n",
        "    le_cols = [c for c in cat_cols if df[c].nunique()>categorical_thresh]\n",
        "    num_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),\n",
        "                                    ('scaler', StandardScaler())])\n",
        "    ohe_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "                                    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "    le_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent'))])\n",
        "    preprocessor = ColumnTransformer(transformers=[\n",
        "        ('num', num_transformer, num_cols),\n",
        "        ('ohe', ohe_transformer, ohe_cols),\n",
        "        ('le', le_transformer, le_cols)\n",
        "    ], remainder='drop')\n",
        "\n",
        "    return preprocessor, num_cols, ohe_cols, le_cols"
      ],
      "metadata": {
        "id": "oZJLzVTTNk3u"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_models(X_train, y_train):\n",
        "    models = {}\n",
        "    lr = LogisticRegression(max_iter=1000, random_state=RANDOM_STATE)\n",
        "    lr.fit(X_train, y_train)\n",
        "    models['LogisticRegression'] = lr\n",
        "    rf = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
        "    rf.fit(X_train, y_train)\n",
        "    models['RandomForest'] = rf\n",
        "\n",
        "    return models"
      ],
      "metadata": {
        "id": "cWaZRwztNuJK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X_test, y_test, model_name='model'):\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_proba = None\n",
        "    try:\n",
        "        y_proba = model.predict_proba(X_test)[:,1]\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
        "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    roc = roc_auc_score(y_test, y_proba) if y_proba is not None else None\n",
        "\n",
        "\n",
        "    print(f'--- {model_name} ---')\n",
        "    print('Accuracy:', acc)\n",
        "    print('Precision:', prec)\n",
        "    print('Recall:', rec)\n",
        "    print('F1:', f1)\n",
        "    if roc is not None:\n",
        "        print('ROC-AUC:', roc)\n",
        "    print('Confusion matrix:\\n', cm)\n",
        "    print('\\nClassification report:\\n', classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "    if y_proba is not None:\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "        plt.figure(); plt.plot(fpr, tpr); plt.plot([0,1],[0,1],'--'); plt.title(f'ROC - {model_name}'); plt.xlabel('FPR'); plt.ylabel('TPR'); plt.show()\n",
        "\n",
        "\n",
        "    return {'accuracy':acc, 'precision':prec, 'recall':rec, 'f1':f1, 'roc_auc':roc, 'confusion_matrix':cm}"
      ],
      "metadata": {
        "id": "ngMTukpbN2nA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_full_pipeline(df, target, test_size=0.3):\n",
        "    assert target in df.columns, 'Target column not found'\n",
        "\n",
        "\n",
        "    df_clean = remove_duplicates(df.copy())\n",
        "    df_clean = impute_missing(df_clean)\n",
        "    df_clean = cap_outliers_iqr(df_clean)\n",
        "\n",
        "    eda_plots(df_clean, target=target)\n",
        "    print('\\nBasic insights:\\n', '\\n'.join(basic_insights(df_clean, target=target)))\n",
        "\n",
        "    preprocessor, num_cols, ohe_cols, le_cols = create_feature_pipeline(df_clean, categorical_thresh=7, target=target)\n",
        "\n",
        "\n",
        "    X = df_clean.drop(columns=[target])\n",
        "    y = df_clean[target]\n",
        "\n",
        "    if y.dtype=='object':\n",
        "        le = LabelEncoder()\n",
        "        y = le.fit_transform(y)\n",
        "\n",
        "    X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=RANDOM_STATE, stratify=y if len(np.unique(y))>1 else None)\n",
        "\n",
        "\n",
        "    X_train = preprocessor.fit_transform(X_train_raw)\n",
        "    X_test = preprocessor.transform(X_test_raw)\n",
        "\n",
        "    models = train_models(X_train, y_train)\n",
        "\n",
        "    results = {}\n",
        "    for name, m in models.items():\n",
        "        results[name] = evaluate_model(m, X_test, y_test, model_name=name)\n",
        "\n",
        "    best_name = max(results.keys(), key=lambda k: results[k]['f1'])\n",
        "    best_model = models[best_name]\n",
        "    joblib.dump({'model':best_model, 'preprocessor':preprocessor}, 'model_pipeline.pkl')\n",
        "    print('Saved best model:', best_name)\n",
        "\n",
        "\n",
        "    return {'models':models, 'results':results, 'best':best_name}"
      ],
      "metadata": {
        "id": "OUd-_UogOBML"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fastapi_snippet = '''\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "app = FastAPI()\n",
        "model_bundle = joblib.load('model_pipeline.pkl')\n",
        "model = model_bundle['model']\n",
        "preprocessor = model_bundle['preprocessor']\n",
        "\n",
        "\n",
        "class InputSchema(BaseModel):\n",
        "    # Update these fields to match your features, example:\n",
        "    Pclass: int\n",
        "    Sex: str\n",
        "    Age: float\n",
        "    SibSp: int\n",
        "    Parch: int\n",
        "    Fare: float\n",
        "    Embarked: str\n",
        "\n",
        "\n",
        "@app.post('/predict')\n",
        "def predict(payload: InputSchema):\n",
        "    data = pd.DataFrame([payload.dict()])\n",
        "    X = preprocessor.transform(data)\n",
        "    y_proba = model.predict_proba(X)[:,1]\n",
        "    y = model.predict(X)\n",
        "    return {'prediction': int(y[0]), 'probability': float(y_proba[0])}\n",
        "'''\n",
        "\n",
        "with open('fastapi_app_snippet.py','w') as f:\n",
        "    f.write(fastapi_snippet)\n",
        "\n",
        "\n",
        "print('FastAPI snippet saved as fastapi_app_snippet.py')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a1R3JV2OOU6",
        "outputId": "5295dc35-594e-4d28-aa00-ec6d6f37fa4b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastAPI snippet saved as fastapi_app_snippet.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"heart.csv\")  # upload your file OR load via URL\n",
        "\n",
        "X = df.drop(\"target\", axis=1)\n",
        "y = df[\"target\"]\n",
        "\n",
        "pipe = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"model\", RandomForestClassifier())\n",
        "])\n",
        "\n",
        "pipe.fit(X, y)\n",
        "\n",
        "# Save model\n",
        "joblib.dump(pipe, \"model_pipeline.pkl\")\n",
        "\n",
        "print(\"model_pipeline.pkl saved\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CE3iDJYXKTF",
        "outputId": "e4e82dc7-0b51-4b42-9dda-011fb1585946"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_pipeline.pkl saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"model_pipeline.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "PH80T7BzXWKg",
        "outputId": "2af46ad8-237c-4cb9-d62f-2fec80034ad9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_29bcb247-eb62-4140-b716-75b6765bccf8\", \"model_pipeline.pkl\", 1274690)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}